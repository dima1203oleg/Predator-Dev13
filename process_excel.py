import os
from datetime import datetime
from typing import List

import redis
import requests  # For Ollama integration
from neo4j import GraphDatabase
from opensearchpy import OpenSearch
from opensearchpy import helpers as opensearch_helpers
from qdrant_client.models import Distance
from sqlalchemy.orm import Session

from api.database import Base, SessionLocal, engine
from api.models import Dataset, Record
from api.qdrant_manager import QdrantManager
from parsers.excel_parser import ExcelParser

# Ensure all tables are created (if not already)
Base.metadata.create_all(bind=engine)

class MultiDatabaseIndexer:
    """–Ü–Ω–¥–µ–∫—Å—É—î –¥–∞–Ω—ñ —É –≤—Å—ñ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –∑ –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—î—é"""

    def __init__(self, db_session: Session = None):
        # Configuration from environment variables
        self.opensearch_host = os.getenv("OPENSEARCH_HOST", "localhost")
        self.opensearch_user = os.getenv("OPENSEARCH_USER", "admin")
        self.opensearch_password = os.getenv("OPENSEARCH_PASSWORD", "admin")
        self.qdrant_host = os.getenv("QDRANT_HOST", "localhost")
        self.qdrant_port = int(os.getenv("QDRANT_PORT", 6333))
        self.qdrant_collection = os.getenv("QDRANT_COLLECTION", "customs_records_v1")
        self.qdrant_vector_size = int(os.getenv("QDRANT_VECTOR_SIZE", 768))
        self.neo4j_uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
        self.neo4j_user = os.getenv("NEO4J_USER", "neo4j")
        self.neo4j_password = os.getenv("NEO4J_PASSWORD", "password")
        self.redis_host = os.getenv("REDIS_HOST", "localhost")
        self.redis_port = int(os.getenv("REDIS_PORT", 6379))
        self.ollama_url = os.getenv("OLLAMA_URL", "http://localhost:11434")
        self.ollama_embed_model = os.getenv("OLLAMA_EMBED_MODEL", "nomic-embed-text")

        # Database session for DLQ logging
        self.db_session = db_session

        # OpenSearch
        self.opensearch = OpenSearch(
            hosts=[{"host": self.opensearch_host, "port": 9200}],
            http_auth=(self.opensearch_user, self.opensearch_password),
            use_ssl=False,
            verify_certs=False,
            ssl_show_warn=False
        )

        # Qdrant
        self.qdrant_manager = QdrantManager(
            host=self.qdrant_host,
            port=self.qdrant_port,
            collection_name=self.qdrant_collection
        )

        # Neo4j
        self.neo4j_driver = GraphDatabase.driver(
            self.neo4j_uri,
            auth=(self.neo4j_user, self.neo4j_password)
        )

        # Redis
        self.redis_client = redis.Redis(
            host=self.redis_host,
            port=self.redis_port,
            db=0,
            decode_responses=True
        )

        print("‚úÖ Initialized all database connections")

    def setup_opensearch_aliases(self):
        """–°—Ç–≤–æ—Ä—é—î aliases –¥–ª—è OpenSearch: write/read/safe"""
        try:
            index_name = "customs_records"

            # Write alias (–¥–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó)
            write_alias = {
                "actions": [
                    {
                        "add": {
                            "index": index_name,
                            "alias": "pa-customs-write",
                            "is_write_index": True
                        }
                    }
                ]
            }

            # Read alias (–¥–ª—è –ø–æ—à—É–∫—É)
            read_alias = {
                "actions": [
                    {
                        "add": {
                            "index": index_name,
                            "alias": "pa-customs-read"
                        }
                    }
                ]
            }

            # Safe alias (–∑ PII –º–∞—Å–∫—É–≤–∞–Ω–Ω—è–º)
            safe_alias = {
                "actions": [
                    {
                        "add": {
                            "index": index_name,
                            "alias": "pa-customs-safe"
                        }
                    }
                ]
            }

            # Apply aliases
            for alias_name, alias_body in [
                ("write", write_alias),
                ("read", read_alias),
                ("safe", safe_alias)
            ]:
                try:
                    self.opensearch.indices.update_aliases(body=alias_body)
                    print(f"‚úÖ OpenSearch alias 'pa-customs-{alias_name}' created")
                except Exception as e:
                    print(f"‚ö†Ô∏è Failed to create alias 'pa-customs-{alias_name}': {e}")

        except Exception as e:
            print(f"‚ö†Ô∏è Error setting up OpenSearch aliases: {e}")

    def setup_opensearch_pii_pipeline(self):
        """–°—Ç–≤–æ—Ä—é—î ingest pipeline –¥–ª—è PII –º–∞—Å–∫—É–≤–∞–Ω–Ω—è"""
        try:
            pipeline_body = {
                "description": "Mask PII fields for safe alias",
                "processors": [
                    {
                        "set": {
                            "field": "edrpou_masked",
                            "value": "***",
                            "if": "ctx.edrpou != null"
                        }
                    },
                    {
                        "set": {
                            "field": "company_name_masked",
                            "value": "REDACTED",
                            "if": "ctx.company_name != null"
                        }
                    },
                    {
                        "remove": {
                            "field": ["edrpou", "company_name"],
                            "if": "ctx._index == 'pa-customs-safe'"
                        }
                    }
                ]
            }

            self.opensearch.ingest.put_pipeline(
                id="pii_masking_pipeline",
                body=pipeline_body
            )
            print("‚úÖ OpenSearch PII masking pipeline created")

        except Exception as e:
            print(f"‚ö†Ô∏è Failed to create PII pipeline: {e}")

    def _log_index_error(self, record: Record, target_db: str, operation: str, error: Exception):
        """–õ–æ–≥—É—î –ø–æ–º–∏–ª–∫–∏ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó —É DLQ —Ç–∞–±–ª–∏—Ü—é"""
        try:

            from api.models import IndexError

            # Get db session from indexer
            db_session = getattr(self, 'db_session', None)
            if not db_session:
                print(f"‚ö†Ô∏è Cannot log DLQ error - no db session: {target_db}/{operation}: {error}")
                return

            dlq_entry = IndexError(
                record_id=record.id,
                target_db=target_db,
                operation=operation,
                error_message=str(error),
                retry_count=0
            )
            db_session.add(dlq_entry)
            db_session.commit()

        except Exception as dlq_error:
            print(f"‚ö†Ô∏è Failed to log DLQ error: {dlq_error}")

    def reindex_from_postgres(self, dataset_id: str, batch_size: int = 1000):
        """–ü–æ–≤–Ω–∏–π reindex –∑ PostgreSQL —É –≤—Å—ñ —Ü—ñ–ª—å–æ–≤—ñ –ë–î"""
        try:
            from api.models import Record

            print(f"üîÑ Starting full reindex for dataset {dataset_id}")

            # –û—Ç—Ä–∏–º—É—î–º–æ –≤—Å—ñ –∑–∞–ø–∏—Å–∏ –¥–∞—Ç–∞—Å–µ—Ç—É
            records = self.db_session.query(Record).filter(Record.dataset_id == dataset_id).all()
            total_records = len(records)

            print(f"üìä Found {total_records} records to reindex")

            # Reindex —É –±–∞—Ç—á–∞—Ö
            for i in range(0, total_records, batch_size):
                batch = records[i:i + batch_size]
                print(f"  Processing batch {i//batch_size + 1}/{(total_records + batch_size - 1)//batch_size}")

                # OpenSearch
                os_count = self.index_to_opensearch(batch)

                # Qdrant (–ø–µ—Ä–µ–≥–µ–Ω–µ—Ä—É—î–º–æ embeddings)
                qdrant_count = self.index_to_qdrant(batch)

                # Neo4j
                neo4j_count = self.index_to_neo4j(batch)

                # Redis
                redis_count = self.cache_to_redis(batch)

                print(f"    Batch results: OS={os_count}, Qdrant={qdrant_count}, Neo4j={neo4j_count}, Redis={redis_count}")

            print(f"‚úÖ Reindex completed for dataset {dataset_id}")
            return True

        except Exception as e:
            print(f"‚ùå Reindex failed: {e}")
            return False

    def consistency_check(self, dataset_id: str, sample_size: int = 100):
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ –º—ñ–∂ –ë–î"""
        try:
            from api.models import Record

            print(f"üîç Starting consistency check for dataset {dataset_id}")

            # –ü—ñ–¥—Ä–∞—Ö—É—î–º–æ –≤ PG
            pg_count = self.db_session.query(Record).filter(Record.dataset_id == dataset_id).count()
            print(f"üìä PostgreSQL: {pg_count} records")

            # OpenSearch
            try:
                os_result = self.opensearch.count(index="customs_records", body={"query": {"match_all": {}}})
                os_count = os_result['count']
                print(f"üîç OpenSearch: {os_count} documents")
            except Exception as e:
                print(f"‚ö†Ô∏è OpenSearch count failed: {e}")
                os_count = 0

            # Qdrant
            try:
                qdrant_info = self.qdrant_manager.get_collection_info()
                qdrant_count = qdrant_info.get('points_count', 0)
                print(f"üßÆ Qdrant: {qdrant_count} vectors")
            except Exception as e:
                print(f"‚ö†Ô∏è Qdrant count failed: {e}")
                qdrant_count = 0

            # Neo4j (–ø—ñ–¥—Ä–∞—Ö—É—î–º–æ IMPORTS –∑–≤'—è–∑–∫–∏)
            try:
                with self.neo4j_driver.session() as session:
                    result = session.run("MATCH ()-[r:IMPORTS]-() RETURN count(r) as count")
                    neo4j_count = result.single()['count']
                    print(f"üï∏Ô∏è Neo4j: {neo4j_count} IMPORTS relationships")
            except Exception as e:
                print(f"‚ö†Ô∏è Neo4j count failed: {e}")
                neo4j_count = 0

            # Sample check (–ø–æ—Ä—ñ–≤–Ω—é—î–º–æ op_hash)
            if sample_size > 0:
                sample_records = self.db_session.query(Record).filter(Record.dataset_id == dataset_id).limit(sample_size).all()
                mismatches = 0

                for record in sample_records:
                    record_id = str(record.id)

                    # Check OpenSearch
                    try:
                        os_doc = self.opensearch.get(index="customs_records", id=record_id)
                        if os_doc['_source']['op_hash'] != record.op_hash:
                            mismatches += 1
                    except Exception:
                        mismatches += 1

                    # Check Qdrant
                    try:
                        qdrant_results = self.qdrant_manager.search_similar([0.1] * 768, limit=1, filters={"pk": record.pk})
                        if not qdrant_results or qdrant_results[0]['payload']['op_hash'] != record.op_hash:
                            mismatches += 1
                    except Exception:
                        mismatches += 1

                print(f"üîç Sample check ({sample_size} records): {mismatches} mismatches")

            # Summary
            print("\nüìà Consistency Summary:")
            print(f"  PostgreSQL: {pg_count}")
            print(f"  OpenSearch: {os_count} ({'‚úì' if os_count == pg_count else '‚úó'})")
            print(f"  Qdrant: {qdrant_count} ({'‚úì' if qdrant_count == pg_count else '‚úó'})")
            print(f"  Neo4j: {neo4j_count} ({'‚úì' if neo4j_count == pg_count else '‚úó'})")

            return {
                'pg_count': pg_count,
                'os_count': os_count,
                'qdrant_count': qdrant_count,
                'neo4j_count': neo4j_count,
                'consistent': pg_count == os_count == qdrant_count == neo4j_count
            }

        except Exception as e:
            print(f"‚ùå Consistency check failed: {e}")
            return None

    def _embed_text(self, text: str) -> List[float]:
        """Generates embeddings for a given text using Ollama."""
        try:
            r = requests.post(
                f"{self.ollama_url}/api/embeddings",
                json={"model": self.ollama_embed_model, "prompt": text},
                timeout=60
            )
            r.raise_for_status()
            return r.json()["embedding"]
        except requests.exceptions.RequestException as e:
            print(f"‚ö†Ô∏è Ollama embedding error: {e}")
            return [] # Return empty list on error

    def _make_text_for_embedding(self, record: Record) -> str:
        """
        Concatenates informative fields from a record to create text for embedding.
        Handles None values and trims extra spaces.
        """
        parts = [
            record.company_name,
            record.hs_code,
            record.customs_office,
            record.edrpou,
            record.country_code
        ]
        # Filter out None values and join with a single space
        return " ".join(filter(None, parts)).strip()

    def ensure_opensearch_index(self):
        """–°—Ç–≤–æ—Ä—é—î —ñ–Ω–¥–µ–∫—Å OpenSearch —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î"""
        index_name = "customs_records"

        if not self.opensearch.indices.exists(index=index_name):
            index_body = {
                "settings": {
                    "number_of_shards": 2,
                    "number_of_replicas": 1,
                    "analysis": {
                        "analyzer": {
                            "ukrainian_analyzer": {
                                "type": "standard",
                                "stopwords": "_ukrainian_"
                            }
                        }
                    }
                },
                "mappings": {
                    "properties": {
                        "record_id": {"type": "keyword"},
                        "op_hash": {"type": "keyword"},
                        "hs_code": {"type": "keyword"},
                        "date": {"type": "date"},
                        "amount": {"type": "double"},
                        "qty": {"type": "double"},
                        "country_code": {"type": "keyword"},
                        "edrpou": {"type": "keyword"},
                        "company_name": {
                            "type": "text",
                            "analyzer": "ukrainian_analyzer",
                            "fields": {
                                "keyword": {"type": "keyword"}
                            }
                        },
                        "customs_office": {
                            "type": "text",
                            "analyzer": "ukrainian_analyzer",
                            "fields": {
                                "keyword": {"type": "keyword"}
                            }
                        },
                        "full_text": {"type": "text", "analyzer": "ukrainian_analyzer"},
                        "indexed_at": {"type": "date"}
                    }
                }
            }

            self.opensearch.indices.create(index=index_name, body=index_body)
            print(f"‚úÖ Created OpenSearch index: {index_name}")
        else:
            print(f"‚úì OpenSearch index already exists: {index_name}")

    def ensure_qdrant_collection(self):
        """–°—Ç–≤–æ—Ä—é—î –∫–æ–ª–µ–∫—Ü—ñ—é Qdrant —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î"""
        try:
            self.qdrant_manager.create_collection(
                vector_size=self.qdrant_vector_size, # Use configured vector size
                distance=Distance.COSINE,
                recreate=False
            )
            print("‚úÖ Qdrant collection ready")
        except Exception as e:
            print(f"‚ö†Ô∏è Qdrant collection setup: {e}")

    def index_to_opensearch(self, records: List[Record]):
        """–Ü–Ω–¥–µ–∫—Å—É—î –∑–∞–ø–∏—Å–∏ —É OpenSearch"""
        if not records:
            return 0

        actions = []
        record_map = {}  # Map action index to record

        for idx, record in enumerate(records):
            # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ–≤–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
            full_text = f"{record.company_name or ''} {record.hs_code or ''} {record.customs_office or ''} {record.edrpou or ''}"

            doc = {
                "_index": "customs_records",
                "_id": str(record.id),
                "_source": {
                    "record_id": str(record.id),
                    "op_hash": record.op_hash,
                    "hs_code": record.hs_code,
                    "date": record.date.isoformat() if record.date else None,
                    "amount": float(record.amount) if record.amount else None,
                    "qty": float(record.qty) if record.qty else None,
                    "country_code": record.country_code,
                    "edrpou": record.edrpou,
                    "company_name": record.company_name,
                    "customs_office": record.customs_office,
                    "full_text": full_text,
                    "indexed_at": datetime.now().isoformat()
                }
            }
            actions.append(doc)
            record_map[idx] = record

        success, failed = opensearch_helpers.bulk(self.opensearch, actions, raise_on_error=False)

        # Log failed items to DLQ
        if isinstance(failed, list) and failed:
            for fail_item in failed:
                action_idx = fail_item.get('index', 0)
                if action_idx in record_map:
                    error_msg = fail_item.get('error', 'Unknown error')
                    self._log_index_error(
                        record_map[action_idx],
                        'opensearch',
                        'bulk_index',
                        Exception(str(error_msg))
                    )

        print(f"‚úÖ OpenSearch: indexed {success} records, failed {len(failed) if isinstance(failed, list) else 0}")
        return success

    def index_to_qdrant(self, records: List[Record]):
        """–Ü–Ω–¥–µ–∫—Å—É—î –∑–∞–ø–∏—Å–∏ —É Qdrant (–≤–µ–∫—Ç–æ—Ä–Ω–∏–π –ø–æ—à—É–∫)"""
        if not records:
            return 0

        points = []
        failed_records = []

        for record in records:
            try:
                text_for_embedding = self._make_text_for_embedding(record)
                embedding = self._embed_text(text_for_embedding)

                if not embedding:
                    print(f"‚ö†Ô∏è Skipping Qdrant indexing for record {record.id} due to embedding error.")
                    self._log_index_error(record, 'qdrant', 'embed', Exception("Empty embedding returned"))
                    failed_records.append(record)
                    continue

                point = {
                    "id": str(record.id),
                    "vector": embedding,
                    "payload": {
                        "pk": record.pk,
                        "title": f"{record.company_name} - {record.hs_code}",
                        "tags": ["customs", record.country_code or "unknown"],
                        "meta": {
                            "hs_code": record.hs_code,
                            "amount": float(record.amount) if record.amount else 0,
                            "date": record.date.isoformat() if record.date else None
                        }
                    }
                }
                points.append(point)

            except Exception as e:
                print(f"‚ö†Ô∏è Failed to prepare Qdrant point for record {record.id}: {e}")
                self._log_index_error(record, 'qdrant', 'prepare_point', e)
                failed_records.append(record)

        if points:
            stats = self.qdrant_manager.upsert_vectors(points, batch_size=100)
            print(f"‚úÖ Qdrant: upserted {stats['upserted']} vectors, skipped {stats['skipped']}, failed {stats['failed']}")
            return stats['upserted']
        return 0

    def index_to_neo4j(self, records: List[Record]):
        """–Ü–Ω–¥–µ–∫—Å—É—î –∑–∞–ø–∏—Å–∏ —É Neo4j (–≥—Ä–∞—Ñ–æ–≤–∞ –ë–î)"""
        if not records:
            return 0

        indexed = 0
        with self.neo4j_driver.session() as session:
            for record in records:
                try:
                    # –°—Ç–≤–æ—Ä—é—î–º–æ –≤—É–∑–ª–∏ —Ç–∞ –∑–≤'—è–∑–∫–∏
                    query = """
                    MERGE (c:Company {edrpou: $edrpou, name: $company_name})
                    MERGE (p:Product {hs_code: $hs_code})
                    MERGE (co:Country {code: $country_code})
                    MERGE (c)-[r:IMPORTS {
                        amount: $amount,
                        qty: $qty,
                        date: date($date),
                        record_id: $record_id
                    }]->(p)
                    MERGE (p)-[:FROM_COUNTRY]->(co)
                    """

                    session.run(query, {
                        "edrpou": record.edrpou or "UNKNOWN",
                        "company_name": record.company_name or "Unknown Company",
                        "hs_code": record.hs_code or "0000",
                        "country_code": record.country_code or "XX",
                        "amount": float(record.amount) if record.amount else 0.0,
                        "qty": float(record.qty) if record.qty else 0.0,
                        "date": record.date.isoformat() if record.date else "2024-01-01",
                        "record_id": str(record.id)
                    })
                    indexed += 1
                except Exception as e:
                    print(f"‚ö†Ô∏è Neo4j error for record {record.id}: {e}")
                    self._log_index_error(record, 'neo4j', 'merge_graph', e)

        print(f"‚úÖ Neo4j: created {indexed} graph nodes/relationships")
        return indexed

    def cache_to_redis(self, records: List[Record]):
        """–ö–µ—à—É—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —É Redis"""
        if not records:
            return 0

        # –ö–µ—à—É—î–º–æ –∞–≥—Ä–µ–≥–æ–≤–∞–Ω—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        pipeline = self.redis_client.pipeline()
        cached_count = 0

        for record in records:
            # –ö–µ—à –ø–æ HS –∫–æ–¥—É
            if record.hs_code:
                key = f"hs_code:{record.hs_code}:count"
                pipeline.incr(key)
                pipeline.expire(key, 3600)  # 1 –≥–æ–¥–∏–Ω–∞
                cached_count += 1

            # –ö–µ—à –ø–æ –∫–æ–º–ø–∞–Ω—ñ—ó
            if record.edrpou:
                key = f"company:{record.edrpou}:total_amount"
                pipeline.incrbyfloat(key, float(record.amount) if record.amount else 0)
                pipeline.expire(key, 3600)
                cached_count += 1 # Count each key update as a cached item

        pipeline.execute()
        print(f"‚úÖ Redis: cached statistics for {cached_count} operations")
        return cached_count

    def close(self):
        """–ó–∞–∫—Ä–∏–≤–∞—î –≤—Å—ñ –∑'—î–¥–Ω–∞–Ω–Ω—è"""
        try:
            self.neo4j_driver.close()
            self.redis_client.close()
            print("‚úÖ Closed all database connections")
        except Exception as e:
            print(f"‚ö†Ô∏è Error closing connections: {e}")


def process_excel_file(file_path: str, dataset_name: str, owner: str = "system_user"):
    """
    –û–±—Ä–æ–±–ª—è—î Excel —Ñ–∞–π–ª —Ç–∞ —ñ–Ω–¥–µ–∫—Å—É—î —É –≤—Å—ñ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö:
    1. PostgreSQL - —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ
    2. OpenSearch - –ø–æ–≤–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–∏–π –ø–æ—à—É–∫
    3. Qdrant - –≤–µ–∫—Ç–æ—Ä–Ω–∏–π –ø–æ—à—É–∫
    4. Neo4j - –≥—Ä–∞—Ñ–æ–≤—ñ –∑–≤'—è–∑–∫–∏
    5. Redis - –∫–µ—à —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    """
    db: Session = SessionLocal()
    indexer = MultiDatabaseIndexer(db_session=db)

    try:
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ —ñ–Ω–¥–µ–∫—Å–∏/–∫–æ–ª–µ–∫—Ü—ñ—ó
        print("\nüîß Initializing database schemas...")
        indexer.ensure_opensearch_index()
        indexer.ensure_qdrant_collection()

        # 1. Create a new Dataset
        print(f"\nüìä Creating dataset: {dataset_name}")
        dataset = Dataset(
            name=dataset_name,
            type="customs",
            description=f"Customs data from {os.path.basename(file_path)}",
            schema_json={},
            owner=owner,
            status="active",
        )
        db.add(dataset)
        db.commit()
        db.refresh(dataset)
        print(f"‚úÖ Created Dataset with ID: {dataset.id}")

        # 2. Parse the Excel file
        print("\nüìù Parsing Excel file...")
        parser = ExcelParser()
        parse_result = parser.parse(file_path)
        records_data = parse_result["records"]
        print(f"‚úÖ Parsed {len(records_data)} valid records")

        pg_processed = 0
        pg_failed = 0
        pg_duplicates = 0
        new_records_batch = []

        # 3. Insert parsed records into PostgreSQL
        print("\nüíæ Inserting into PostgreSQL...")
        for record_data in records_data:
            try:
                # Check for duplicates based on op_hash
                existing = (
                    db.query(Record).filter(Record.op_hash == record_data.get("op_hash")).first()
                )

                if existing:
                    pg_duplicates += 1
                    continue

                record = Record(
                    dataset_id=dataset.id,
                    pk=record_data["pk"],
                    op_hash=record_data["op_hash"],
                    hs_code=record_data.get("hs_code"),
                    date=record_data.get("date"),
                    amount=record_data.get("amount"),
                    qty=record_data.get("qty"),
                    country_code=record_data.get("country_code"),
                    edrpou=record_data.get("edrpou"),
                    company_name=record_data.get("company_name"),
                    customs_office=record_data.get("customs_office"),
                    attrs=record_data,
                    source_file=os.path.basename(file_path),
                    source_row=int(record_data["pk"].split('_')[-1]),
                )
                db.add(record)
                new_records_batch.append(record)
                pg_processed += 1

                # Commit —É –±–∞—Ç—á–∞—Ö –ø–æ BATCH_SIZE_PG_COMMIT –∑–∞–ø–∏—Å—ñ–≤
                if len(new_records_batch) >= int(os.getenv("BATCH_SIZE_PG_COMMIT", 1000)):
                    db.commit()
                    print(f"  ‚úì Committed batch of {len(new_records_batch)} records to PostgreSQL")
                    new_records_batch = []

            except Exception as e:
                print(f"‚ö†Ô∏è Failed to process record for PostgreSQL: {e}")
                pg_failed += 1

        # Final commit for any remaining records
        if new_records_batch:
            db.commit()
            print(f"  ‚úì Final commit: {len(new_records_batch)} records to PostgreSQL")

        # Update dataset metadata
        dataset.row_count = pg_processed
        dataset.updated_at = datetime.now()
        db.commit()

        # 4. –Ü–Ω–¥–µ–∫—Å—É—î–º–æ —É —ñ–Ω—à—ñ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
        print(f"\nüîÑ Indexing {pg_processed} records to other databases...")

        # –û—Ç—Ä–∏–º—É—î–º–æ –≤—Å—ñ –Ω–æ–≤—ñ –∑–∞–ø–∏—Å–∏ –¥–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó
        all_records_for_indexing = db.query(Record).filter(Record.dataset_id == dataset.id).all()

        opensearch_indexed_count = 0
        qdrant_upserted_count = 0
        neo4j_indexed_count = 0
        redis_cached_count = 0

        # OpenSearch
        print("\nüîç Indexing to OpenSearch...")
        opensearch_indexed_count = indexer.index_to_opensearch(all_records_for_indexing)

        # Qdrant
        print("\nüßÆ Indexing to Qdrant (vector database)...")
        qdrant_upserted_count = indexer.index_to_qdrant(all_records_for_indexing)

        # Neo4j
        print("\nüï∏Ô∏è Indexing to Neo4j (graph database)...")
        neo4j_indexed_count = indexer.index_to_neo4j(all_records_for_indexing)

        # Redis
        print("\n‚ö° Caching to Redis...")
        redis_cached_count = indexer.cache_to_redis(all_records_for_indexing)

        # Final summary
        print(f"\n{'='*60}")
        print("‚úÖ PROCESSING COMPLETE")
        print(f"{'='*60}")
        print(f"Dataset ID: {dataset.id}")
        print(f"Dataset Name: {dataset.name}")
        print(f"Total rows in Excel: {parse_result['total_rows']}")
        print(f"Valid records parsed: {parse_result['valid_rows']}")
        print(f"Records inserted to PostgreSQL: {pg_processed}")
        print(f"Duplicates skipped (PostgreSQL): {pg_duplicates}")
        print(f"Failed records (PostgreSQL): {pg_failed}")
        print("\nüìä Indexed across all databases:")
        print(f"  ‚úì PostgreSQL: {pg_processed} records")
        print(f"  ‚úì OpenSearch: {opensearch_indexed_count} records indexed")
        print(f"  ‚úì Qdrant: {qdrant_upserted_count} vectors upserted")
        print(f"  ‚úì Neo4j: {neo4j_indexed_count} graph nodes/relationships created")
        print(f"  ‚úì Redis: {redis_cached_count} statistics cached")
        print(f"{'='*60}\n")

    except Exception as e:
        print(f"\n‚ùå An error occurred during Excel processing: {e}")
        import traceback
        traceback.print_exc()
    finally:
        indexer.close()
        db.close()

if __name__ == "__main__":
    excel_file_path = "/Users/dima/Desktop/–ë–µ—Ä–µ–∑–µ–Ω—å_2024.xlsx"
    dataset_name = "–ë–µ—Ä–µ–∑–µ–Ω—å_2024_Customs_Declarations"

    if not os.path.exists(excel_file_path):
        print(f"Error: Excel file not found at {excel_file_path}")
    else:
        process_excel_file(excel_file_path, dataset_name)
